{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization with NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OFzmB5PFkRx"
      },
      "source": [
        "# Simple Text Summarization using NLTK\n",
        "_NLTK is the **N**atural **L**anguage **T**ool**k**it library in Python_.\n",
        "> Reference: [How-to article by Usman Malik](https://stackabuse.com/text-summarization-with-nltk-in-python/)\n",
        "\n",
        "This technique uses relative word frequency to score sentences in a long body of text, then ranks those sentences by the highest scores and returns the top sentences. It is a simple way to summarize large amounts of text using the author's own words, and provides a surprisingly smooth summary of key themes in the text.\n",
        "\n",
        "This is an _extractive_ method because the sentences used for the summary come from the text itself.\n",
        "\n",
        "Other techniques of text summarization, called _abstractive_ methods, use machine learning to generate new text summaries based on language patterns and themes of the text.\n",
        "\n",
        "## Uses\n",
        "This Python program will extract brief summaries from large amounts of text. **It is best used with text that has a single theme or message.** For example: \n",
        "* News articles \n",
        "* Academic research papers \n",
        "* Speeches or addresses \n",
        "* Wikipedia articles \n",
        "* Sports updates \n",
        "* Chapters of popular business books ðŸ˜‰\n",
        "\n",
        "## Limitations\n",
        "This technique is not the best for summarizing large bodies of text with multiple themes or an evolving story. For example: \n",
        "* Novels (especially those with complex plots)\n",
        "* Entire books, even if each chapter has a central theme\n",
        "* How-to guides (since the order of instructions is important)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDKLvOXBpPsY"
      },
      "source": [
        "# Setup\n",
        "A user inputs the following settings:\n",
        "* URLs (web pages) to summarize\n",
        "* Summary length (in number of sentences)\n",
        "\n",
        "Also, in this section I import the required packages for the analysis: `requests` for interacting with web sites, `BeautifulSoup` for parsing webpage HTML, `re` for using regular expressions (RegEx) to clean the text, `nltk` for the text analysis, and `heapq` for returning the top sentences by word frequency rank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOhBnvapHaOb"
      },
      "source": [
        "# How long (in sentences) would you like each summary to be?\n",
        "summary_lenth = 6\n",
        "\n",
        "page_to_summarize = 'https://en.wikipedia.org/wiki/Hubble_Ultra-Deep_Field'\n",
        "\n",
        "# In the list below, enter websites to summarize\n",
        "# pages_to_summarize = [\n",
        "#                       'https://speeches.byu.edu/talks/lawrence-e-corbridge/stand-for-ever/', \n",
        "#                       'https://en.wikipedia.org/wiki/Machine_learning', \n",
        "#                       'https://en.wikipedia.org/wiki/Maui', \n",
        "#                       'https://en.wikipedia.org/wiki/Hana_Highway', \n",
        "#                       'https://en.wikipedia.org/wiki/Hubble_Ultra-Deep_Field', \n",
        "#                       'https://en.wikipedia.org/wiki/Milky_Way'\n",
        "#                       ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x33jMgezwEAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835d43c6-8eb1-441a-df9a-75362aafa397"
      },
      "source": [
        "# import required packages\n",
        "import requests                 # for making web requests\n",
        "from bs4 import BeautifulSoup   # for parsing web pages\n",
        "import re                       # for working with regular expressions to clean text\n",
        "import nltk                     # for text analysis\n",
        "import statistics               # for performing basic statistical functions\n",
        "import heapq                    # for returning \"top N\" sentences\n",
        "\n",
        "# get list of stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# get punctuation from NLTK\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKuLM4dtsR6O"
      },
      "source": [
        "# Step-by-Step process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cZnVYbFqM2P"
      },
      "source": [
        "## Get all text from the webpage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OgDTzV4qM9b"
      },
      "source": [
        "webpage = requests.get(page_to_summarize)\n",
        "parsed_page = BeautifulSoup(webpage.text, 'html')\n",
        "# paragraphs = parsed_page.findAll(text=True)       # find all tags that have text, like <div>, <span> or <p>\n",
        "paragraphs = parsed_page.findAll('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for paragraph in paragraphs:\n",
        "    article_text += ' ' + paragraph.text\n",
        "\n",
        "# remove blank spaces before and after the article text\n",
        "article_text = article_text.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCpVwnA8uujJ"
      },
      "source": [
        "## Remove Wikipedia references from the text\n",
        "In Wikipedia articles, references are contained in superscripted brackets following a word, like this: _sample reference_ `[3]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM5-8sNWqND2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "8ffd4a7c-94c9-4224-8c2c-3766399b8fc7"
      },
      "source": [
        "# Use the re (RegEx) library to substitute any references with an empty space\n",
        "# See: https://www.kite.com/python/answers/how-to-use-re.sub()-in-python\n",
        "# Also: https://docs.python.org/3/library/re.html#regular-expression-syntax\n",
        "\n",
        "# The 'r' in front of the pattern tells Python to treat this as a raw string\n",
        "# so any Python-specific character sequences (like /n for a new line)\n",
        "# will be treated as ordinary text.\n",
        "article_text = re.sub(\n",
        "    pattern = r'\\[[0-9]*\\]', # or, to remove characters too: pattern = r'\\[[a-z0-9]*\\]'\n",
        "    repl = ' ', \n",
        "    string = article_text)\n",
        "\n",
        "# replace the extra spaces with a single space\n",
        "article_text = re.sub(\n",
        "    pattern = r'\\s+', \n",
        "    repl = ' ', \n",
        "    string = article_text)\n",
        "\n",
        "article_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Coordinates: 3h 32m 39.0s, âˆ’27Â° 47â€² 29.1â€³ The Hubble Ultra-Deep Field (HUDF) is a deep-field image of a small region of space in the constellation Fornax, containing an estimated 10,000 galaxies. The original data for the image was collected by the Hubble Space Telescope from September 2003 to January 2004. It includes light from galaxies that existed about 13 billion years ago, some 400 to 800 million years after the Big Bang. The HUDF image was taken in a section of the sky with a low density of bright stars in the near-field, allowing much better viewing of dimmer, more distant objects. Located southwest of Orion in the southern-hemisphere constellation Fornax, the rectangular image is 2.4 arcminutes to an edge, or 3.4 arcminutes diagonally. This is about one-tenth of the angular diameter of a full moon viewed from Earth (less than 34 arcminutes), smaller than a 1 mm2 piece of paper held 1 m away, and equal to roughly one twenty-six-millionth of the total area of the sky. The image is oriented so that the upper left corner points toward north (âˆ’46.4Â°) on the celestial sphere. In August and September 2009, the HUDF field was observed at longer wavelengths (1.0 to 1.6 Âµm) using the infrared channel of the recently-fitted Wide Field Camera 3 (WFC3). This additional data enabled astronomers to identify a new list of potentially very distant galaxies. On September 25, 2012, NASA released a new version of the Ultra-Deep Field dubbed the eXtreme Deep Field (XDF). The XDF reveals galaxies from 13.2 billion years ago, including one thought to have formed only 450 million years after the Big Bang. On June 3, 2014, NASA released the Hubble Ultra Deep Field 2014 image, the first HUDF image to use the full range of ultraviolet to near-infrared light. A composite of separate exposures taken in 2002 to 2012 with Hubble's Advanced Camera for Surveys and Wide Field Camera 3, it shows some 10,000 galaxies. On January 23, 2019, the Instituto de AstrofÃ­sica de Canarias released an even deeper version of the infrared images of the Hubble Ultra Deep Field obtained with the WFC3 instrument, named the ABYSS Hubble Ultra Deep Field. The new images improve the previous reduction of the WFC3/IR images, including careful sky background subtraction around the largest galaxies on the field of view. After this update, some galaxies were found to be almost twice as big as previously measured. In the years since the original Hubble Deep Field, the Hubble Deep Field South and the GOODS sample were analyzed, providing increased statistics at the high redshifts probed by the HDF. When the Advanced Camera for Surveys (ACS) detector was installed on the HST, it was realized that an ultra-deep field could observe galaxy formation out to even higher redshifts than had currently been observed, as well as providing more information about galaxy formation at intermediate redshifts (z~2). A workshop on how to best carry out surveys with the ACS was held at STScI in late 2002. At the workshop Massimo Stiavelli advocated an Ultra Deep Field as a way to study the objects responsible for the reionization of the Universe. Following the workshop, the STScI Director Steven Beckwith decided to devote 400 orbits of Director's Discretionary time to the UDF and appointed Stiavelli as the lead of the Home Team implementing the observations. Unlike the Deep Fields, the HUDF does not lie in Hubble's Continuous Viewing Zone (CVZ). The earlier observations, using the Wide Field and Planetary Camera 2 (WFPC2) camera, were able to take advantage of the increased observing time on these zones by using wavelengths with higher noise to observe at times when earthshine contaminated the observations; however, ACS does not observe at these wavelengths, so the advantage was reduced. As with the earlier fields, this one was required to contain very little emission from our galaxy, with little Zodiacal dust. The field was also required to be in a range of declinations such that it could be observed both by southern hemisphere instruments, such as the Atacama Large Millimeter Array, and northern hemisphere ones, such as those located on Hawaii. It was ultimately decided to observe a section of the Chandra Deep Field South, due to existing deep X-ray observations from Chandra X-ray Observatory and two interesting objects already observed in the GOODS sample at the same location: a redshift 5.8 galaxy and a supernova. The coordinates of the field are right ascension 3h 32m 39.0s, declination âˆ’27Â° 47â€² 29.1â€³ (J2000). The field is 200 arcseconds to a side, with a total area of 11 square arcminutes, and lies in the constellation of Fornax. Four filters were used on the ACS, centered on 435, 606, 775 and 850 nm, with exposure times set to give equal sensitivity in all filters. These wavelength ranges match those used by the GOODS sample, allowing direct comparison between the two. As with the Deep Fields, the HUDF used Directors Discretionary Time. In order to get the best resolution possible, the observations were dithered by pointing the telescope at slightly different positions for each exposureâ€”a process trialled with the Hubble Deep Fieldâ€”so that the final image has a higher resolution than the pixels on their own would normally allow. The observations were done in two sessions, from September 23 to October 28, 2003, and December 4, 2003 to January 15, 2004. The total exposure time is just under 1 million seconds, from 400 orbits, with a typical exposure time of 1200 seconds. In total, 800 ACS exposures were taken over the course of 11.3 days, two per orbit; NICMOS observed for 4.5 days. All the individual ACS exposures were processed and combined by Anton Koekemoer into a set of scientifically useful images, each with a total exposure time ranging from 134,900 seconds to 347,100 seconds. To observe the whole sky to the same sensitivity, the HST would need to observe continuously for a million years. The sensitivity of the ACS limits its capability of detecting galaxies at high redshift to about 6. The deep NICMOS fields obtained in parallel to the ACS images could in principle be used to detect galaxies at redshift 7 or higher but they were lacking visible band images of similar depth. These are necessary to identify high redshift objects as they should not be seen in the visible bands. In order to obtain deep visible exposures on top of the NICMOS parallel fields a follow-up program, HUDF05, was approved and granted 204 orbits to observe the two parallel fields (GO-10632). The orientation of the HST was chosen so that further NICMOS parallel images would fall on top of the main UDF field. After the installation of WFC3 on Hubble in 2009, the HUDF09 programme (GO-11563) devoted 192 orbits to observations of three fields, including HUDF, using the newly available F105W, F125W and F160W infra-red filters (which correspond to the Y, J and H bands): The HUDF is the deepest image of the universe ever taken and has been used to search for galaxies that existed between 400 and 800 million years after the Big Bang (redshifts between 7 and 12). Several galaxies in the HUDF are candidates, based on photometric redshifts, to be amongst the most distant astronomical objects. The red dwarf UDF 2457 at distance of 59,000 light-years is the furthest star resolved by the HUDF. The star near the center of the field is USNO-A2.0 0600-01400432 with apparent magnitude of 18.95. [better source needed] The field imaged by the ACS contains over 10,000 objects, the majority of which are galaxies, many at redshifts greater than 3, and some that probably have redshifts between 6 and 7. The NICMOS measurements may have discovered galaxies at redshifts up to 12. The HUDF has revealed high rates of star formation during the very early stages of galaxy formation, within a billion years after the Big Bang. It has also enabled improved characterization of the distribution of galaxies, their numbers, sizes and luminosities at different epochs, aiding investigation into the evolution of galaxies. Galaxies at high redshifts have been confirmed to be smaller and less symmetrical than ones at lower redshifts, illuminating the rapid evolution of galaxies in the first couple of billion years after the Big Bang. The Hubble eXtreme Deep Field (HXDF), released on September 25, 2012, is an image of a portion of space in the center of the Hubble Ultra Deep Field image. Representing a total of two million seconds (about 23 days) of exposure time collected over 10 years, the image covers an area of 2.3 arcminutes by 2 arcminutes, or about 80% of the area of the HUDF. This represents approximately one thirty-two millionth of the sky. The HXDF contains about 5,500 galaxies, the oldest of which are seen as they were 13.2 billion years ago. The faintest galaxies are one ten-billionth the brightness of what the human eye can see. The red galaxies in the image are the remnants of galaxies after major collisions during their elderly years. Many of the smaller galaxies in the image are very young galaxies that eventually developed into major galaxies, similar to the Milky Way and other galaxies in our galactic neighborhood. XDF size compared with the size of the Moon HXDF image shows mature galaxies in the foreground plane, nearly mature galaxies from 5 to 9 billion years ago, and protogalaxies beyond 9 billion years. Video (02:42) about how the Hubble eXtreme Deep Field image was made.\""
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHrgW_fLz-IA"
      },
      "source": [
        "## Copy text, then remove special characters and digits to have words only "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahAWuUSVz-37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "e572075f-eadd-45ab-e1aa-0a43d1fdbd15"
      },
      "source": [
        "# replace non-letter characters with a single space\n",
        "words_only = re.sub(\n",
        "    pattern = r'[^a-zA-Z]', \n",
        "    repl = ' ', \n",
        "    string = article_text)\n",
        "\n",
        "# replace double spaces with a single space\n",
        "words_only = re.sub(\n",
        "    pattern = r'\\s+', \n",
        "    repl = ' ', \n",
        "    string = words_only)\n",
        "\n",
        "words_only"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Coordinates h m s The Hubble Ultra Deep Field HUDF is a deep field image of a small region of space in the constellation Fornax containing an estimated galaxies The original data for the image was collected by the Hubble Space Telescope from September to January It includes light from galaxies that existed about billion years ago some to million years after the Big Bang The HUDF image was taken in a section of the sky with a low density of bright stars in the near field allowing much better viewing of dimmer more distant objects Located southwest of Orion in the southern hemisphere constellation Fornax the rectangular image is arcminutes to an edge or arcminutes diagonally This is about one tenth of the angular diameter of a full moon viewed from Earth less than arcminutes smaller than a mm piece of paper held m away and equal to roughly one twenty six millionth of the total area of the sky The image is oriented so that the upper left corner points toward north on the celestial sphere In August and September the HUDF field was observed at longer wavelengths to m using the infrared channel of the recently fitted Wide Field Camera WFC This additional data enabled astronomers to identify a new list of potentially very distant galaxies On September NASA released a new version of the Ultra Deep Field dubbed the eXtreme Deep Field XDF The XDF reveals galaxies from billion years ago including one thought to have formed only million years after the Big Bang On June NASA released the Hubble Ultra Deep Field image the first HUDF image to use the full range of ultraviolet to near infrared light A composite of separate exposures taken in to with Hubble s Advanced Camera for Surveys and Wide Field Camera it shows some galaxies On January the Instituto de Astrof sica de Canarias released an even deeper version of the infrared images of the Hubble Ultra Deep Field obtained with the WFC instrument named the ABYSS Hubble Ultra Deep Field The new images improve the previous reduction of the WFC IR images including careful sky background subtraction around the largest galaxies on the field of view After this update some galaxies were found to be almost twice as big as previously measured In the years since the original Hubble Deep Field the Hubble Deep Field South and the GOODS sample were analyzed providing increased statistics at the high redshifts probed by the HDF When the Advanced Camera for Surveys ACS detector was installed on the HST it was realized that an ultra deep field could observe galaxy formation out to even higher redshifts than had currently been observed as well as providing more information about galaxy formation at intermediate redshifts z A workshop on how to best carry out surveys with the ACS was held at STScI in late At the workshop Massimo Stiavelli advocated an Ultra Deep Field as a way to study the objects responsible for the reionization of the Universe Following the workshop the STScI Director Steven Beckwith decided to devote orbits of Director s Discretionary time to the UDF and appointed Stiavelli as the lead of the Home Team implementing the observations Unlike the Deep Fields the HUDF does not lie in Hubble s Continuous Viewing Zone CVZ The earlier observations using the Wide Field and Planetary Camera WFPC camera were able to take advantage of the increased observing time on these zones by using wavelengths with higher noise to observe at times when earthshine contaminated the observations however ACS does not observe at these wavelengths so the advantage was reduced As with the earlier fields this one was required to contain very little emission from our galaxy with little Zodiacal dust The field was also required to be in a range of declinations such that it could be observed both by southern hemisphere instruments such as the Atacama Large Millimeter Array and northern hemisphere ones such as those located on Hawaii It was ultimately decided to observe a section of the Chandra Deep Field South due to existing deep X ray observations from Chandra X ray Observatory and two interesting objects already observed in the GOODS sample at the same location a redshift galaxy and a supernova The coordinates of the field are right ascension h m s declination J The field is arcseconds to a side with a total area of square arcminutes and lies in the constellation of Fornax Four filters were used on the ACS centered on and nm with exposure times set to give equal sensitivity in all filters These wavelength ranges match those used by the GOODS sample allowing direct comparison between the two As with the Deep Fields the HUDF used Directors Discretionary Time In order to get the best resolution possible the observations were dithered by pointing the telescope at slightly different positions for each exposure a process trialled with the Hubble Deep Field so that the final image has a higher resolution than the pixels on their own would normally allow The observations were done in two sessions from September to October and December to January The total exposure time is just under million seconds from orbits with a typical exposure time of seconds In total ACS exposures were taken over the course of days two per orbit NICMOS observed for days All the individual ACS exposures were processed and combined by Anton Koekemoer into a set of scientifically useful images each with a total exposure time ranging from seconds to seconds To observe the whole sky to the same sensitivity the HST would need to observe continuously for a million years The sensitivity of the ACS limits its capability of detecting galaxies at high redshift to about The deep NICMOS fields obtained in parallel to the ACS images could in principle be used to detect galaxies at redshift or higher but they were lacking visible band images of similar depth These are necessary to identify high redshift objects as they should not be seen in the visible bands In order to obtain deep visible exposures on top of the NICMOS parallel fields a follow up program HUDF was approved and granted orbits to observe the two parallel fields GO The orientation of the HST was chosen so that further NICMOS parallel images would fall on top of the main UDF field After the installation of WFC on Hubble in the HUDF programme GO devoted orbits to observations of three fields including HUDF using the newly available F W F W and F W infra red filters which correspond to the Y J and H bands The HUDF is the deepest image of the universe ever taken and has been used to search for galaxies that existed between and million years after the Big Bang redshifts between and Several galaxies in the HUDF are candidates based on photometric redshifts to be amongst the most distant astronomical objects The red dwarf UDF at distance of light years is the furthest star resolved by the HUDF The star near the center of the field is USNO A with apparent magnitude of better source needed The field imaged by the ACS contains over objects the majority of which are galaxies many at redshifts greater than and some that probably have redshifts between and The NICMOS measurements may have discovered galaxies at redshifts up to The HUDF has revealed high rates of star formation during the very early stages of galaxy formation within a billion years after the Big Bang It has also enabled improved characterization of the distribution of galaxies their numbers sizes and luminosities at different epochs aiding investigation into the evolution of galaxies Galaxies at high redshifts have been confirmed to be smaller and less symmetrical than ones at lower redshifts illuminating the rapid evolution of galaxies in the first couple of billion years after the Big Bang The Hubble eXtreme Deep Field HXDF released on September is an image of a portion of space in the center of the Hubble Ultra Deep Field image Representing a total of two million seconds about days of exposure time collected over years the image covers an area of arcminutes by arcminutes or about of the area of the HUDF This represents approximately one thirty two millionth of the sky The HXDF contains about galaxies the oldest of which are seen as they were billion years ago The faintest galaxies are one ten billionth the brightness of what the human eye can see The red galaxies in the image are the remnants of galaxies after major collisions during their elderly years Many of the smaller galaxies in the image are very young galaxies that eventually developed into major galaxies similar to the Milky Way and other galaxies in our galactic neighborhood XDF size compared with the size of the Moon HXDF image shows mature galaxies in the foreground plane nearly mature galaxies from to billion years ago and protogalaxies beyond billion years Video about how the Hubble eXtreme Deep Field image was made '"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z96V2cA3z_A_"
      },
      "source": [
        "## Determine weighted frequency of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI3cJRCI0-2c"
      },
      "source": [
        "# Get list of stopwords; words with little meaning, like \"the\", \"an\", \"is\", etc.\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# Create a dictionary to hold the words and their frequencies\n",
        "word_frequencies = {}\n",
        "\n",
        "# loop through the words and create a unique list of words and their counts\n",
        "for word in words_only.lower().split(sep = ' '):\n",
        "    # make sure the word isn't a meaningless stopword\n",
        "    if word not in stopwords:\n",
        "        # if the word isn't in the frequency dictionary yet, add it\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "# Alternative form of the loop:\n",
        "# for word in nltk.word_tokenize(words_only.lower()):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next steps: implement a TF-IDF (term frequency times inverse document frequency) measure on each term, defining \"documents\" as sentences or perhaps as paragraphs of text in the page."
      ],
      "metadata": {
        "id": "IUWAoebS_Lo4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUaeBc683TUn"
      },
      "source": [
        "# Update the word frequency count to a weighted frequency\n",
        "\n",
        "# Determine the maximum frequency\n",
        "max_frequency = max(word_frequencies.values())\n",
        "\n",
        "# Loop through the dictionary and update the value of each word\n",
        "# to its weighted frequency (that is, word_count Ã· max_frequency)\n",
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = (word_frequencies[word] / max_frequency)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHXGxShPMdCG"
      },
      "source": [
        "Sort the dictionary of word frequencies to facilitate ranking by order. This way, we can add extra points to sentences that begin with one of the top words.\n",
        "\n",
        "Note here: We can choose an arbitrary \"top _n_\" number of words to use, or we can calculate breaks that determine what the top words are. Here a few methods:\n",
        "* [pandas `qcut`](https://pbpython.com/pandas-qcut-cut.html): divide the data into bins with the same number of data points. For example, use 20 bins and take the top one for the top 5% of words\n",
        "* Python's built-in `statistics` package has a [`.quantile()`](https://docs.python.org/3/library/statistics.html#statistics.quantiles) function that performs the same technique (though not as robust) as pandas `qcut`\n",
        "* Use [Jenk's algorithm to determine natural breaks](https://pbpython.com/natural-breaks.html) in the data and find the top group of words. This technique produces breaks that appear intuitive: for example, if the 7th-most used word has a weighted frequency of 0.5 and the 8th-most frequent word's frequency is 0.38, with the next 10 words at a similar frequency, we could determine that the top group would be the 7 most-used words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E9hTI5gMEbp"
      },
      "source": [
        "sorted_word_freq = sorted(word_frequencies.items(), key = lambda item: item[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L987W7ozMROJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6be67f96-fba0-47e9-e8c7-b2a3ae041e15"
      },
      "source": [
        "# Set top word count to 10 for more than 200 unique words \n",
        "# or to 5% of the number of unique words\n",
        "if len(sorted_word_freq) < 200:\n",
        "    top_word_count = int(len(sorted_word_freq) * .05)\n",
        "else:\n",
        "    top_word_count = 10\n",
        "\n",
        "top_words = [word[0] for word in sorted_word_freq[:top_word_count]]\n",
        "\n",
        "print(top_word_count)\n",
        "print(top_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "['field', 'galaxies', 'deep', 'image', 'years', 'hubble', 'hudf', 'redshifts', 'acs', 'ultra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqPckILmHwxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2c19d1-5bc8-4382-f27e-0c98f6533df1"
      },
      "source": [
        "# Using natural breaks\n",
        "\n",
        "!pip install jenkspy\n",
        "import jenkspy\n",
        "\n",
        "breaks = jenkspy.jenks_breaks(values=word_frequencies.values(), nb_class=2)\n",
        "print(breaks)\n",
        "\n",
        "top_word_freq = {}\n",
        "for item in sorted_word_freq:\n",
        "    top_word_freq[item[0]] = item[1] >= breaks[1]\n",
        "\n",
        "top_word_freq = [item[0] for item in top_word_freq.items() if item[1] == True]\n",
        "top_word_freq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jenkspy in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "[0.03571428571428571, 0.35714285714285715, 1.0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['field', 'galaxies', 'deep', 'image', 'years', 'hubble', 'hudf', 'redshifts']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpQEcNDaMoOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82329f58-73c9-45a9-da1b-f4784f4de32a"
      },
      "source": [
        "sorted_word_freq[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('field', 1.0),\n",
              " ('galaxies', 0.9642857142857143),\n",
              " ('deep', 0.75),\n",
              " ('image', 0.5714285714285714),\n",
              " ('years', 0.5357142857142857),\n",
              " ('hubble', 0.5),\n",
              " ('hudf', 0.5),\n",
              " ('redshifts', 0.35714285714285715),\n",
              " ('acs', 0.32142857142857145),\n",
              " ('ultra', 0.2857142857142857)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PkZCq_k0--d"
      },
      "source": [
        "## Split text into sentences (i.e., tokenize by sentence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wq6b1xEz_JO"
      },
      "source": [
        "# Use the full article text (with references removed, but punctuation and numbers intact)\n",
        "# to generate a list of the sentences in the article\n",
        "sentence_list = nltk.sent_tokenize(article_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqhtEJKv1Fm9"
      },
      "source": [
        "## Calculate sentence scores\n",
        "These scores are based on the sum of the weighted frequencies of the words in the sentence. \n",
        "\n",
        "Note that stopwords, numbers, and special characters have a weighted frequency of 0 since they weren't included in the distinct word list used to calculate the weighted frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtNyzR249PWl"
      },
      "source": [
        "# Set a maximum length of the sentences allowed to be used in the summary.\n",
        "# Note that sentences with more words could be scored higher simply\n",
        "# by having more words. This attempts to reduce that effect.\n",
        "\n",
        "# Create a list of the word counts of each sentence\n",
        "sentence_word_counts = [len(sent.split(' ')) for sent in sentence_list]\n",
        "\n",
        "# Find the maximum, average, and median sentence lengths\n",
        "max_word_count = max(sentence_word_counts)\n",
        "avg_word_count = sum(sentence_word_counts) / len(sentence_list)\n",
        "median_word_count = statistics.median(sentence_word_counts)\n",
        "\n",
        "# Find the standard deviation of sentence length\n",
        "stdev_word_count = statistics.stdev(sentence_word_counts)\n",
        "\n",
        "# Set the maximum summary sentence length to the median sentence length\n",
        "# max_summary_sentence_length = median_word_count\n",
        "\n",
        "# Set the maximum summary sentence length to the avg sentence length\n",
        "# max_summary_sentence_length = avg_word_count\n",
        "\n",
        "# Set the maximum summary sentence length to 1 stdev above the mean\n",
        "max_summary_sentence_length = int(avg_word_count + stdev_word_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCaXWu5g1FxZ"
      },
      "source": [
        "# create a dictionary of the sentence scores\n",
        "sentence_scores = {}\n",
        "score_multiplier = 1\n",
        "# loop through the sentences and score them\n",
        "for sent in sentence_list:\n",
        "    # skip sentences longer than the maximum word count for summary sentences\n",
        "    # if len(nltk.word_tokenize(sent)) > 30: #max_summary_sentence_length:\n",
        "    #     continue\n",
        "    if len(sent.split(' ')) > max_summary_sentence_length:\n",
        "        continue\n",
        "    \n",
        "    i = 0       # reset word count\n",
        "    \n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        # check if the word is one of the scored words\n",
        "        if word in word_frequencies.keys():\n",
        "            # give double points to sentences with a top word among the first 5 words\n",
        "            if (i < 5) and (word in top_words):\n",
        "                score_multiplier = 2\n",
        "            else:\n",
        "                score_multiplier = 1\n",
        "            i += 1\n",
        "            \n",
        "            # check if the sentence does not yet have a score assigned\n",
        "            if sent not in sentence_scores.keys():\n",
        "                sentence_scores[sent] = (word_frequencies[word]) * score_multiplier\n",
        "            else: \n",
        "                sentence_scores[sent] += (word_frequencies[word]) * score_multiplier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdM53pd1_U9q"
      },
      "source": [
        "## Sort the sentences in descending order by sentence score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYN7Nv0Z7S3j"
      },
      "source": [
        "# Sort the sentence scores dictionary in descending order by word frequency score\n",
        "sorted_sentences = sorted(sentence_scores.items(), key = lambda item: item[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAy7ei4aC25Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "e42f78a5-1116-4a88-b369-0050d888207b"
      },
      "source": [
        "# Alternate version using the heapq library\n",
        "sorted_sentences_v2 = heapq.nlargest(summary_lenth, sentence_scores, key=sentence_scores.get)\n",
        "article_summary_v2 = ' '.join(sorted_sentences_v2)\n",
        "\n",
        "article_summary_v2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Hubble eXtreme Deep Field (HXDF), released on September 25, 2012, is an image of a portion of space in the center of the Hubble Ultra Deep Field image. In the years since the original Hubble Deep Field, the Hubble Deep Field South and the GOODS sample were analyzed, providing increased statistics at the high redshifts probed by the HDF. Coordinates: 3h 32m 39.0s, âˆ’27Â° 47â€² 29.1â€³ The Hubble Ultra-Deep Field (HUDF) is a deep-field image of a small region of space in the constellation Fornax, containing an estimated 10,000 galaxies. Many of the smaller galaxies in the image are very young galaxies that eventually developed into major galaxies, similar to the Milky Way and other galaxies in our galactic neighborhood. On January 23, 2019, the Instituto de AstrofÃ­sica de Canarias released an even deeper version of the infrared images of the Hubble Ultra Deep Field obtained with the WFC3 instrument, named the ABYSS Hubble Ultra Deep Field. Galaxies at high redshifts have been confirmed to be smaller and less symmetrical than ones at lower redshifts, illuminating the rapid evolution of galaxies in the first couple of billion years after the Big Bang.'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj4BtIlD_Tdy"
      },
      "source": [
        "## Create the summary paragraph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHkJdqFi_b7N"
      },
      "source": [
        "article_summary = ''\n",
        "\n",
        "for sent in sorted_sentences[0:summary_lenth]:\n",
        "    article_summary += ' ' + sent[0]\n",
        "\n",
        "# remove whitespace from the beginning and end of the article summary\n",
        "article_summary = article_summary.replace('\\n', ' ')\n",
        "article_summary = article_summary.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFzRqhMi_cDd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "dad09571-243a-492b-8118-91feaa5dff04"
      },
      "source": [
        "article_summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Hubble eXtreme Deep Field (HXDF), released on September 25, 2012, is an image of a portion of space in the center of the Hubble Ultra Deep Field image. In the years since the original Hubble Deep Field, the Hubble Deep Field South and the GOODS sample were analyzed, providing increased statistics at the high redshifts probed by the HDF. Coordinates: 3h 32m 39.0s, âˆ’27Â° 47â€² 29.1â€³ The Hubble Ultra-Deep Field (HUDF) is a deep-field image of a small region of space in the constellation Fornax, containing an estimated 10,000 galaxies. Many of the smaller galaxies in the image are very young galaxies that eventually developed into major galaxies, similar to the Milky Way and other galaxies in our galactic neighborhood. On January 23, 2019, the Instituto de AstrofÃ­sica de Canarias released an even deeper version of the infrared images of the Hubble Ultra Deep Field obtained with the WFC3 instrument, named the ABYSS Hubble Ultra Deep Field. Galaxies at high redshifts have been confirmed to be smaller and less symmetrical than ones at lower redshifts, illuminating the rapid evolution of galaxies in the first couple of billion years after the Big Bang.'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPoylR86ASiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b815c56-75ae-4bc5-bbbd-e4ef0704440d"
      },
      "source": [
        "print(article_summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Hubble eXtreme Deep Field (HXDF), released on September 25, 2012, is an image of a portion of space in the center of the Hubble Ultra Deep Field image. In the years since the original Hubble Deep Field, the Hubble Deep Field South and the GOODS sample were analyzed, providing increased statistics at the high redshifts probed by the HDF. Coordinates: 3h 32m 39.0s, âˆ’27Â° 47â€² 29.1â€³ The Hubble Ultra-Deep Field (HUDF) is a deep-field image of a small region of space in the constellation Fornax, containing an estimated 10,000 galaxies. Many of the smaller galaxies in the image are very young galaxies that eventually developed into major galaxies, similar to the Milky Way and other galaxies in our galactic neighborhood. On January 23, 2019, the Instituto de AstrofÃ­sica de Canarias released an even deeper version of the infrared images of the Hubble Ultra Deep Field obtained with the WFC3 instrument, named the ABYSS Hubble Ultra Deep Field. Galaxies at high redshifts have been confirmed to be smaller and less symmetrical than ones at lower redshifts, illuminating the rapid evolution of galaxies in the first couple of billion years after the Big Bang.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIpFKwb7qNJ5"
      },
      "source": [
        "# Single-function process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68pVQ7FhFuxU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}