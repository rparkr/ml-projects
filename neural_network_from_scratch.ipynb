{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "In this notebook, I implement a configurable \"vanilla\" fully connected neural network from scratch, then use it to create text embeddings.\n",
    "\n",
    "I built this project to practice translating mathematical operations (e.g., forward and backward prop, gradient descent) into vectorized code.\n",
    "\n",
    "Author: [Ryan Parker](https://github.com/rparkr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in packages\n",
    "import random  # pseudorandom number generation for setting seed values\n",
    "from typing import Union, Optional  # type annotations for functions\n",
    "\n",
    "# third-party packages\n",
    "import numpy as np  # array-based computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    '''Calculate probabilities using the sigmoid function on input data, z.\n",
    "    The sigmoid, or logistic, function is used as an activation for\n",
    "    neurons of a neural network because it is non-linear.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z: {float, np.ndarray}\n",
    "        The input to be passed through the sigmoid function.\n",
    "        Typically, input values come from a linear combination\n",
    "        of input values with weights and a bias term. That is,\n",
    "        some form of y = wx + b.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid_value: {float, np.ndarray}\n",
    "        The output of the sigmoid function, in the same\n",
    "        shape as the input.\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_df():\n",
    "    pass\n",
    "\n",
    "\n",
    "def relu(x_input: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    '''Compute the Rectified Linear Unit activation function.'''\n",
    "    return np.maximum(x_input, 0)\n",
    "\n",
    "\n",
    "def linear(\n",
    "        x_input: np.ndarray,\n",
    "        weights: np.ndarray,\n",
    "        bias: Union[float, np.ndarray] = 0.0) -> np.ndarray:\n",
    "    '''Compute a linear function: y = wx + b.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_hat: the computed value(s) after multiplying\n",
    "        inputs by the weights and adding a bias.\n",
    "    '''\n",
    "    # Matrix-multiplication with the @ operator\n",
    "    y_hat = (x_input @ weights) + bias\n",
    "    return y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network architecture\n",
    "The model is a fully connected network, with configurable parameters for the number of layers and the number of neurons per layer. To enable that flexibility, I've chosen to implement the model as a class that can be instantiated into a neural network object that can be trained and used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn():\n",
    "    '''Create a fully connected neural network with a configurable number of layers and neurons.'''\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim: int,\n",
    "            n_layers: int = 2, \n",
    "            neurons_per_layer: Union[int, list[int]] = [3, 1], \n",
    "            bias: Union[bool, list[bool]] = True,\n",
    "            seed: int = random.randint(0, 99_999)):\n",
    "        '''Instantiate a neural network as a model object that performs\n",
    "        computations on input data and returns the outputs\n",
    "        of those computations. Parameters (weights and biases) are\n",
    "        intitialized to random values with mean 0 and standard deviation of 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            The dimension of the inputs to the model (i.e, the number\n",
    "            of features per input sample).\n",
    "        n_layers: int, default = 2\n",
    "            The number of layers in the network including all hidden layers\n",
    "            and the output layer.\n",
    "        neurons_per_layer: {int, list[int]}, default = [3, 1]\n",
    "            The number of neurons in each layer of the network. If an `int`,\n",
    "            then all layers will have the same number of neurons. If a\n",
    "            `list`, then its `len()` must equal `n_layers`, where each\n",
    "            element in the list holds the number of neurons in the \n",
    "            corresponding layer of the network.\n",
    "        bias: {bool, list[bool]}, default = True\n",
    "            Whether to include a bias term for the neurons in the network.\n",
    "            If `True`, all neurons in the network will have a bias term.\n",
    "            If a list of `bool` values is provided, each layer of the network\n",
    "            will (or won't) have a bias term for its neurons based on the\n",
    "            corresponding element in the list. If a list, the number of\n",
    "            elements must equal `n_layers`. \n",
    "        seed: int\n",
    "            For reproducibility, set a seed that will be used\n",
    "            when initializing parameter values. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: an instance of `nn` with randomly-initialized weights.\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        forward(input): compute a forward pass through the network,\n",
    "            returning the output from the final layer.\n",
    "        backward(): compute a backward pass through the network,\n",
    "            storing the partial derivatives of the parameters\n",
    "            with respect to the loss.\n",
    "        zero_grad(): clears (zeros-out) the stored gradients.\n",
    "        step(learning_rate): update the parameters by taking\n",
    "            a step (scaled by learning_rate) in the negative\n",
    "            direction of the gradient.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        w: a list of the network's layers, in order.\n",
    "            Each layer is a numpy array of weights.\n",
    "        b: a list of the bias values in each layer of the network,\n",
    "            in order.\n",
    "        n_params: the total number of parameters in the network,\n",
    "            including weights and biases.\n",
    "        gradients: a list of the gradients for each of the parameters\n",
    "            in the network, computed after calling backward().\n",
    "        input_dim: the size of the input that will be passed into the\n",
    "            network. The network will expect all inputs to have this\n",
    "            same dimension. Since each input sample is a vector, this\n",
    "            is the number of features in an input row.\n",
    "        n_layers: The number of layers in the network, including hidden\n",
    "            layers and the output layer (excluding the input).\n",
    "        neurons_per_layer: An integer or a list of integers representing\n",
    "            the number of neurons in each layer.\n",
    "        bias: A boolean value or a list of boolean values representing\n",
    "            whether each layer has a bias term added to it.\n",
    "        '''\n",
    "        \n",
    "        # Validate arguments\n",
    "        if type(input_dim) != int:\n",
    "            raise TypeError(f\"input_dim must be an integer, but the provided value was: {type(input_dim)}\")\n",
    "\n",
    "        if type(n_layers) != int:\n",
    "            raise TypeError(f\"n_layers must be an integer, but the provided value was: {type(n_layers)}\")\n",
    "        \n",
    "        if type(neurons_per_layer) not in (int, list):\n",
    "            raise TypeError(f\"neurons_per_layer must be an int or a list of int, not: {type(neurons_per_layer)}\")\n",
    "        elif type(neurons_per_layer) == list:\n",
    "            if len(neurons_per_layer) != n_layers:\n",
    "                raise ValueError(f\"If neurons_per_layer is a list, it must have the same number of elements as n_layers ({n_layers}).\")\n",
    "            if any([type(i) != int for i in neurons_per_layer]):\n",
    "                raise TypeError(f\"neurons_per_layer must be an int or a list of int. Not all elements provided in neurons_per_layer were of the int type.\")\n",
    "        \n",
    "        if type(bias) not in (bool, list):\n",
    "            raise TypeError(f\"bias must be a bool or a list of bool, not {type(bias)}\")\n",
    "        elif type(bias) == list:\n",
    "            if len(bias) != n_layers:\n",
    "                raise ValueError(f\"If bias is a list, it must have the same number of elements as n_layers ({n_layers}).\")\n",
    "            if any([type(i) != bool for i in bias]):\n",
    "                raise TypeError(f\"bias must be a bool or a list of bool. Not all elements provided in bias were of the bool type.\")\n",
    "\n",
    "        if type(seed) != int:\n",
    "            raise TypeError(f\"Seed must be an int value, not {type(seed)}\")\n",
    "\n",
    "        # Update object's parameters (equivalent to self.n_layers = n_layers; self.bias = bias; ...)\n",
    "        self.__dict__.update(locals())\n",
    "\n",
    "        # If neurons_per_layer and bias are individual values, convert them \n",
    "        # to lists to use when creating layers\n",
    "        if type(neurons_per_layer) == int:\n",
    "            neurons_per_layer = [neurons_per_layer for _ in range(n_layers)]\n",
    "        if type(bias) == bool:\n",
    "            bias = [bias for _ in range(n_layers)]\n",
    "    \n",
    "        # Create layers (parameters)\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        input_sizes = [input_dim]\n",
    "        input_sizes.extend(neurons_per_layer[:-1])\n",
    "        for n in range(n_layers):\n",
    "            w, b = self._linear_layer(\n",
    "                input_sizes[n], neurons_per_layer[n], bias[n], seed=seed)\n",
    "            self.w.append(w)\n",
    "            self.b.append(b)\n",
    "        \n",
    "        # The second term counts bias terms only for layers with a bias\n",
    "        self.n_params = (np.dot(input_sizes, neurons_per_layer) \n",
    "                         + np.dot(bias, neurons_per_layer))\n",
    "\n",
    "\n",
    "    def _linear_layer(self, input_size: int, n_neurons: int, bias: bool, seed: int):\n",
    "        '''Create a linear layer. Used when constructing the network\n",
    "        at time of instantiation.\n",
    "        \n",
    "        The input size determines the number of weights (rows) and the\n",
    "        number of neurons determines the number of columns. The bias\n",
    "        will be a 1D NumPy array with len() equal to the n_neurons.\n",
    "        '''\n",
    "        # set up random number generator\n",
    "        rng = np.random.default_rng(seed)\n",
    "        # Generate the weight and bias arrays\n",
    "        w = rng.normal(size=(input_size, n_neurons))\n",
    "        if bias:\n",
    "            b = rng.normal(size=(n_neurons))\n",
    "        else:\n",
    "            b = 0\n",
    "        return w, b\n",
    "\n",
    "\n",
    "    def forward(self, input: Union[float, np.ndarray], activation: str='relu'):\n",
    "        '''Pass inputs through the network and return the outputs from\n",
    "        the final network layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input: {float, np.ndarray}\n",
    "            Input to the network, either a single sample or a batch of samples\n",
    "            with shape: m_samples, n_features_per_sample.\n",
    "        activation: {'sigmoid', 'relu'}, default='relu'\n",
    "            The activation function to use after passing input through each\n",
    "            linear layer.\n",
    "        '''\n",
    "        # Validate input\n",
    "        input_size = input.shape[1] if input.ndim > 1 else input.shape[0]\n",
    "        if input_size != self.input_dim:\n",
    "            raise ValueError(f\"Passed input of size {input_size} does not match\"\n",
    "                             f\" expected size of input_dim: {self.input_dim}.\")\n",
    "        \n",
    "        # Compute at each layer and pass to the next\n",
    "        x = input\n",
    "        for n in range(self.n_layers):\n",
    "            y_hat = linear(x_input=x, weights=self.w[n], bias=self.b[n])\n",
    "            if activation == 'sigmoid':\n",
    "                y_hat = sigmoid(y_hat)\n",
    "            else:\n",
    "                y_hat = relu(y_hat)\n",
    "            # Set input for next layer as output of previous\n",
    "            x = y_hat\n",
    "        \n",
    "        return y_hat\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn(input_dim=10, n_layers=4, neurons_per_layer=[3, 4, 6, 2], bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61917585, 0.25379785],\n",
       "       [0.61917596, 0.25379776],\n",
       "       [0.61917596, 0.25379776]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input = np.arange(30).reshape(3, 10)\n",
    "model.forward(x_input, activation='sigmoid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a1d72d1d2bd69df5fbb242d8d66be7c2c6ff0c2599bcae97545e6959cba1f2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
