{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "In this notebook, I implement a configurable fully connected neural network from scratch, then use it to create text embeddings.\n",
    "\n",
    "I built this project to practice translating mathematical concepts (e.g., forward and backward prop, gradient descent) into vectorized code. As such, I relied only on math walkthroughs and did not reference anyone else's code. Two of my favorite resources were:\n",
    "- [3Blue1Brown: Backpropagation Calculus](https://www.3blue1brown.com/lessons/backpropagation-calculus)\n",
    "- [Backprop scroll-through visualization](https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll/)\n",
    "\n",
    "Author: [Ryan Parker](https://github.com/rparkr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in packages\n",
    "from typing import Union  # type annotations for functions\n",
    "\n",
    "# third-party packages\n",
    "import numpy as np  # array-based computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "In this section, I implement functions and their derivatives for use in the forward and backward passes of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    '''Calculate probabilities using the sigmoid function on input data, z.\n",
    "    The sigmoid, or logistic, function is used as an activation for\n",
    "    neurons of a neural network because it is non-linear.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z: {float, np.ndarray}\n",
    "        The input to be passed through the sigmoid function.\n",
    "        Typically, input values come from a linear combination\n",
    "        of input values with weights and a bias term. That is,\n",
    "        some form of y = wx + b.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid_value: {float, np.ndarray}\n",
    "        The output of the sigmoid function, in the same\n",
    "        shape as the input.\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def df_sigmoid(z: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    '''Compute the derivative of the sigmoid function,\n",
    "    which is sigmoid(z) * (1-sigmoid(z)).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z: {float, np.ndarray}\n",
    "        The input to be passed through the sigmoid function.\n",
    "        Typically, input values come from a linear combination\n",
    "        of input values with weights and a bias term. That is,\n",
    "        some form of y = wx + b.\n",
    "    '''\n",
    "    # See: https://beckernick.github.io/sigmoid-derivative-neural-network/\n",
    "    out = sigmoid(z)\n",
    "    return out * (1 - out)\n",
    "\n",
    "\n",
    "def relu(x_input: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    '''Compute the Rectified Linear Unit activation function.'''\n",
    "    return np.maximum(x_input, 0)\n",
    "\n",
    "\n",
    "def df_relu(x_input: np.ndarray) -> np.ndarray:\n",
    "    '''Compute the derivative of the ReLU activation function,\n",
    "    which is 0 when x < 0 and 1 when x > 0.'''\n",
    "    # First, set everything to zeros\n",
    "    out = np.zeros_like(x_input)\n",
    "    # Then set to 1 where x_input > 0\n",
    "    out[x_input > 0] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def linear(\n",
    "        x_input: np.ndarray,\n",
    "        weights: np.ndarray,\n",
    "        bias: np.ndarray) -> np.ndarray:\n",
    "    '''Compute a linear function: y = wx + b.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_hat: the computed value(s) after multiplying\n",
    "        inputs by the weights and adding a bias.\n",
    "    '''\n",
    "    # Matrix-multiplication with the @ operator\n",
    "    y_hat = (x_input @ weights) + bias\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def mse_loss(y_hat: np.ndarray, y_target: np.ndarray) -> float:\n",
    "    '''Compute the mean squared error loss function.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    y_hat: array of shape (m_samples,)\n",
    "        Predictions output from the model.\n",
    "    y_target: array of shape (m_samples,)\n",
    "        The true labels, or targets to predict.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    loss: float\n",
    "        A measure of the average distance between predicted and target values.\n",
    "    '''\n",
    "    num_samples = y_hat.shape[0]\n",
    "    # Below, dividing by 2m is the same as multiplying by (1/2m)\n",
    "    return np.sum((y_hat - y_target) ** 2) / (2 * num_samples)\n",
    "\n",
    "\n",
    "def df_mse_loss(\n",
    "    y_hat: np.ndarray, y_target: np.ndarray) -> np.ndarray:\n",
    "    '''Compute the derivative of mean-squared error loss with respect\n",
    "    to the final outputs of the network. Returns the gradients of the\n",
    "    activations from the final layer of the network.\n",
    "\n",
    "    Used for regression problems, where the target is a continuous\n",
    "    variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_hat: array of shape (m_samples,)\n",
    "        The network's outputs (predictions).\n",
    "    y_target: array of shape (m_samples,)\n",
    "        The labels being predicted, or \"ground truth.\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    derivatives: array of shape (m_samples,)\n",
    "        The derivative of the mean-squared error loss with respect\n",
    "        to the output from the network.\n",
    "    '''\n",
    "    return y_hat - y_target\n",
    "\n",
    "\n",
    "def softmax(x_input: np.ndarray, axis: int=1) -> np.ndarray:\n",
    "    '''Compute the softmax function on an input array.\n",
    "    \n",
    "    Softmax creates a probability distribution across\n",
    "    the input, where rank ordering is preserved but\n",
    "    absolute values change such that the sum of all\n",
    "    probability values output from softmax equals 1.\n",
    "\n",
    "    Equation: e^x / sum(e^x) for all classes\n",
    "    '''\n",
    "    # e^x for all x\n",
    "    e_x = np.exp(x_input)\n",
    "    # Align the sum as a column using [:, np.newaxis]\n",
    "    return e_x / np.sum(e_x, axis=axis)[:, np.newaxis]\n",
    "\n",
    "\n",
    "def df_softmax(x_input: np.ndarray) -> np.ndarray:\n",
    "    '''Compute the derivative of the softmax function \n",
    "    with respecct to an input array.\n",
    "    \n",
    "    Equation:\n",
    "    For input index i = output index j:\n",
    "        df_softmax = softmax(x_i) * (1 - softmax(x_i))\n",
    "    For input position i != output position j:\n",
    "        df_softmax = -softmax(x_i) * softmax(x_j)\n",
    "    '''\n",
    "    # First, the case where i != j\n",
    "    # ...\n",
    "    # We don't actually need to compute this,\n",
    "    # because the derivative of cross entropy loss\n",
    "    # with respect to the softmax outputs already \n",
    "    # takes this into account.\n",
    "    pass\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_hat: np.ndarray, y_target: np.ndarray) -> float:\n",
    "    '''Compute the cross-entropy loss over the network's outputs.\n",
    "\n",
    "    Equation: -sum(y_target * log(y_hat)) for all samples in the batch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_hat: array of shape (m_samples, n_classes)\n",
    "        The network's outputs (predictions), after being\n",
    "        passed through softmax to return a probability\n",
    "        distribution across the classes.\n",
    "    y_target: array of shape (m_samples, n_classes)\n",
    "        A one-hot coded array where each row is a sample\n",
    "        and each column is a possible class, with a 1\n",
    "        in the position of the correct class for that\n",
    "        sample.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss: float\n",
    "        The cross-entropy loss for the given batch.\n",
    "    \n",
    "    '''\n",
    "    # Transpose y_hat (.T) to swap the axes to (n_classes, m_samples)\n",
    "    # for the matrix multiplication to work properly.\n",
    "    return -np.sum(y_target @ np.log(y_hat.T))\n",
    "\n",
    "\n",
    "def df_cross_entropy_loss(\n",
    "        y_hat: np.ndarray, y_target: np.ndarray) -> np.ndarray:\n",
    "    '''Compute the derivative of the cross-entropy loss funtion\n",
    "    with respect to the output of the softmax function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_hat: array of shape (m_samples, n_classes)\n",
    "        The network's outputs (predictions), after being\n",
    "        passed through softmax to return a probability\n",
    "        distribution across the classes.\n",
    "    y_target: array of shape (m_samples, n_classes)\n",
    "        A one-hot coded array where each row is a sample\n",
    "        and each column is a possible class, with a 1\n",
    "        in the position of the correct class for that\n",
    "        sample.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    derivatives: array of shape (m_samples, n_classes)\n",
    "        The derivative of cross-entropy loss with respect\n",
    "        to the output from the softmax function.\n",
    "    '''\n",
    "    return y_hat - y_target\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "The model is a fully connected network, with configurable parameters for the number of layers and the number of neurons per layer. To enable that flexibility, I've chosen to implement the model as a class that can be instantiated into a neural network object that can be trained and used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn():\n",
    "    '''Create a fully connected neural network with a configurable number of layers and neurons.'''\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim: int,\n",
    "            n_layers: int, \n",
    "            neurons_per_layer: Union[int, list[int]], \n",
    "            bias: Union[bool, list[bool]] = True,\n",
    "            activation: str='relu',\n",
    "            lambda_: float = 0.0,\n",
    "            loss_fn: str = 'cross_entropy',\n",
    "            alpha: float = 0.001,\n",
    "            dtype: np.dtype = np.float32,\n",
    "            seed: Union[int, None] = None):\n",
    "        '''Instantiate a neural network as a model object that performs\n",
    "        computations on input data and returns the outputs\n",
    "        of those computations. Parameters (weights and biases) are\n",
    "        intitialized to normally-distributed random values with mean 0 \n",
    "        and standard deviation of 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            The dimension of the inputs to the model (i.e, the number\n",
    "            of features per input sample).\n",
    "        n_layers: int\n",
    "            The number of layers in the network including all hidden layers\n",
    "            and the output layer.\n",
    "        neurons_per_layer: {int, list[int]}\n",
    "            The number of neurons in each layer of the network. If an `int`,\n",
    "            then all layers will have the same number of neurons. If a\n",
    "            `list`, then its `len()` must equal `n_layers`, where each\n",
    "            element in the list holds the number of neurons in the \n",
    "            corresponding layer of the network.\n",
    "        bias: {bool, list[bool]}, default = True\n",
    "            Whether to include a bias term for the neurons in the network.\n",
    "            If `True`, all neurons in the network will have a bias term.\n",
    "            If a list of `bool` values is provided, each layer of the network\n",
    "            will (or won't) have a bias term for its neurons based on the\n",
    "            corresponding element in the list. If a list, the number of\n",
    "            elements must equal `n_layers`.\n",
    "        activation: {'relu', 'sigmoid'}, default='relu'\n",
    "            The activation function to use after passing input through each\n",
    "            linear layer.\n",
    "        lambda_: float in the range [0.0, inf), default=0.0\n",
    "            The weighting factor for the regularizing\n",
    "            term in the loss calculation. If set to 0.0,\n",
    "            no regularization will be used. The larger\n",
    "            this value, the more weights are penalized\n",
    "            and the smaller the weights will be after\n",
    "            training through gradient descent.\n",
    "        loss_fn: {'cross_entropy', 'ce', 'mean_squared_error', 'mse'}\n",
    "            The loss function used for the network. Valid values are\n",
    "            'cross_entropy' or 'mean_squared_error' or an abbreviation.\n",
    "            Use 'mean_squared_error' for regression tasks and set the\n",
    "            output layer to a single neuron.\n",
    "            Use 'cross_entropy' for classification tasks and set\n",
    "            the number of output neurons to the number of possible classes.\n",
    "            Softmax is automatically applied after the output layer when\n",
    "            using 'cross_entropy' loss.\n",
    "        alpha: float, default=0.001\n",
    "            The learning rate, which scales the step size taken during\n",
    "            each pass through gradient descent. Gradients are calculated\n",
    "            in the nn.backward() method and descent happens in nn.step().\n",
    "        dtype: np.dtype, default=np.float32\n",
    "            The data type to use for network parameters. Inputs to the network\n",
    "            should also be in this same data type. NumPy's default is float64,\n",
    "            but most neural networks use float32 or float16 precision to save\n",
    "            memory and computation resources.\n",
    "        seed: int, default=None\n",
    "            For reproducibility, set a seed that will be used\n",
    "            when initializing parameter values. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: an instance of `nn` with randomly-initialized weights.\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        forward(input): compute a forward pass through the network,\n",
    "            returning the output from the final layer.\n",
    "        backward(input, target): compute a backward pass through the network,\n",
    "            storing the partial derivatives of the parameters\n",
    "            with respect to the loss.\n",
    "        zero_grad(): clears (zeros-out) the stored gradients.\n",
    "        step(): update the parameters by taking a step (scaled by alpha)\n",
    "            in the negative direction of the gradient.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        params: a dict of the network's parameters by layer\n",
    "        n_params: the total number of parameters in the network,\n",
    "            including weights and biases.\n",
    "        grad: a dict of the gradients for each of the parameters\n",
    "            in the network, computed after calling backward().\n",
    "        outputs: a dict of the intermediate outputs after each layer,\n",
    "            including inputs to activation functions and the activations\n",
    "            after passing through activation functions.\n",
    "            Used for backpropagation.\n",
    "        input_dim: the size of the input that will be passed into the\n",
    "            network. The network will expect all inputs to have this\n",
    "            same dimension. Since each input sample is a vector, this\n",
    "            is the number of features in an input row.\n",
    "        n_layers: The number of layers in the network, including hidden\n",
    "            layers and the output layer (excluding the input).\n",
    "        neurons_per_layer: An integer or a list of integers representing\n",
    "            the number of neurons in each layer.\n",
    "        bias: A boolean value or a list of boolean values representing\n",
    "            whether each layer has a bias term added to it.\n",
    "        '''\n",
    "        \n",
    "        # Validate arguments\n",
    "        if type(input_dim) != int:\n",
    "            raise TypeError(f\"input_dim must be an integer, but the provided value was: {type(input_dim)}\")\n",
    "\n",
    "        if type(n_layers) != int:\n",
    "            raise TypeError(f\"n_layers must be an integer, but the provided value was: {type(n_layers)}\")\n",
    "        \n",
    "        if type(neurons_per_layer) not in (int, list):\n",
    "            raise TypeError(f\"neurons_per_layer must be an int or a list of int, not: {type(neurons_per_layer)}\")\n",
    "        elif type(neurons_per_layer) == list:\n",
    "            if len(neurons_per_layer) != n_layers:\n",
    "                raise ValueError(f\"If neurons_per_layer is a list, it must have the same number of elements as n_layers ({n_layers}).\")\n",
    "            if any([type(i) != int for i in neurons_per_layer]):\n",
    "                raise TypeError(\"neurons_per_layer must be an int or a list of int. Not all elements provided in neurons_per_layer were of the int type.\")\n",
    "        \n",
    "        if type(bias) not in (bool, list):\n",
    "            raise TypeError(f\"bias must be a bool or a list of bool, not {type(bias)}\")\n",
    "        elif type(bias) == list:\n",
    "            if len(bias) != n_layers:\n",
    "                raise ValueError(f\"If bias is a list, it must have the same number of elements as n_layers ({n_layers}).\")\n",
    "            if any([type(i) != bool for i in bias]):\n",
    "                raise TypeError(\"bias must be a bool or a list of bool. Not all elements provided in bias were of the bool type.\")\n",
    "        \n",
    "        if activation not in ('relu, sigmoid'):\n",
    "            raise ValueError(f\"activation must be a str, either 'relu' or 'sigmoid', not {activation}\")\n",
    "\n",
    "        if type(lambda_) != float:\n",
    "            raise TypeError(f\"The regularizing strength parameter, lambda_, must be of type `float`, not {type(lambda_)}\")\n",
    "        \n",
    "        if type(loss_fn) != str:\n",
    "            raise TypeError(f\"loss_fn must be a str value, not {type(loss_fn)}\")\n",
    "        elif loss_fn[0] not in ['c', 'm']:\n",
    "            raise ValueError(f\"loss_fn must be 'cross_entropy', 'mean_squared_error', or an abbreviation, not {loss_fn}\")\n",
    "\n",
    "        if type(alpha) != float:\n",
    "            raise TypeError(f\"The learning rate parameter, alpha, must be of type `float`, not {type(alpha)}\")\n",
    "\n",
    "        # Update object's parameters (equivalent to self.n_layers = n_layers; self.bias = bias; ...)\n",
    "        self.__dict__.update(locals())\n",
    "\n",
    "        # If neurons_per_layer and bias are individual values, convert them \n",
    "        # to lists to use when creating layers\n",
    "        if type(neurons_per_layer) == int:\n",
    "            neurons_per_layer = [neurons_per_layer for _ in range(n_layers)]\n",
    "        if type(bias) == bool:\n",
    "            bias = [bias for _ in range(n_layers)]\n",
    "    \n",
    "        # Create layers (parameters).\n",
    "        # If you want to calculate gradients through the inputs, start these\n",
    "        # dicts at 0 and update the iteration loop in the backward() function\n",
    "        # to go all the way to the 0th layer (rather than layer 1).\n",
    "        self.params = {}\n",
    "        self.grad = {}\n",
    "        input_sizes = [input_dim]\n",
    "        input_sizes.extend(neurons_per_layer[:-1])\n",
    "        for n in range(n_layers):\n",
    "            p = self._linear_layer(\n",
    "                input_sizes[n], neurons_per_layer[n], bias[n], seed=seed)\n",
    "            self.params[n+1] = p\n",
    "            # Initialize gradients to 0s\n",
    "            self.grad[n+1] = np.zeros_like(p)\n",
    "                \n",
    "        # The second expression counts bias terms only for layers with a bias\n",
    "        self.n_params = (np.dot(input_sizes, neurons_per_layer) \n",
    "                         + np.dot(bias, neurons_per_layer))\n",
    "\n",
    "\n",
    "    def _linear_layer(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            n_neurons: int,\n",
    "            bias: bool,\n",
    "            seed: Union[int, None]):\n",
    "        '''Create a linear layer. Used when constructing the network\n",
    "        at time of instantiation.\n",
    "        \n",
    "        The input size determines the number of weights (rows) and the\n",
    "        number of neurons determines the number of columns.\n",
    "        \n",
    "        The bias is a 1D NumPy array with len() equal to the n_neurons and is\n",
    "        appended as a final row in the weight matrix.\n",
    "        \n",
    "        During the forward pass, a column of `1`s is added to the input so,\n",
    "        when multiplied by the weight matrix, those `1`s line up with the final\n",
    "        row in the weight matrix, which are the bias terms. That has the same\n",
    "        effect as multiplying the input by the weight matrix and then adding\n",
    "        the bias vector, which would be broadcast down the rows.\n",
    "        \n",
    "        This structure simplifies backpropagation: only the weight matrices\n",
    "        need to be calculated and updated, since the weight matrices include\n",
    "        the bias terms in the last row.\n",
    "        '''\n",
    "        # set up random number generator\n",
    "        rng = np.random.default_rng(seed)\n",
    "        # Generate the weight and bias arrays\n",
    "        w = rng.normal(size=(input_size, n_neurons))\n",
    "        if bias:\n",
    "            b = rng.normal(size=(n_neurons))\n",
    "        else:\n",
    "            b = np.zeros(shape=(n_neurons))\n",
    "        # Add the bias as the last row in the weight matrix, prepending\n",
    "        # a dimension to b for the concatenation.\n",
    "        # Also convert to the float32 data type, which saves memory\n",
    "        # over NumPy's default float64. In practice, float32 and float16\n",
    "        # are the most common data types for neural network parameters.\n",
    "        return np.concatenate((w, b[np.newaxis, :]), axis=0).astype(self.dtype)\n",
    "\n",
    "\n",
    "    def forward(self, input: np.ndarray):\n",
    "        '''Pass inputs through the network and return the outputs from\n",
    "        the final network layer.\n",
    "\n",
    "        During the forward pass, a column of `1`s is added to the input,\n",
    "        as if there were an extra neuron in the input with an activation\n",
    "        of 1, such that when multiplied by the weight matrix whose final\n",
    "        row has bias terms, the effect is the same as multiplying the\n",
    "        input matrix by the weight matrix and then adding a row vector of\n",
    "        bias terms, which would be broadcast down the columns.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input: np.ndarray\n",
    "            Input to the network, either a single sample or a batch of samples\n",
    "            with shape (m_samples, n_features).\n",
    "        '''\n",
    "        # Validate input\n",
    "        input_size = input.shape[1] if input.ndim > 1 else input.shape[0]\n",
    "        if input_size != self.input_dim:\n",
    "            raise ValueError(f\"Passed input of size {input_size} does not match\"\n",
    "                             f\" expected size of input_dim: {self.input_dim}.\")\n",
    "        \n",
    "        if self.activation not in ('relu', 'sigmoid'):\n",
    "            print(\n",
    "                f\"WARNING: Activation function, '{self.activation}', is not one of the acceptable values: \"\n",
    "                \"('relu', 'sigmoid'), so the default ('relu') is being used. Use \"\n",
    "                \"nn.activation = 'relu' or nn.activation = 'sigmoid' \"\n",
    "                \"to change this setting.\")\n",
    "        # Store intermediate outputs to use with backprop, starting with\n",
    "        # the input layer. 'input' is what goes into the activation function\n",
    "        # for a given layer: the layer's inputs multiplied by its weights\n",
    "        outputs = {\n",
    "            0: {'input': input, 'activation': input}\n",
    "        }\n",
    "\n",
    "        # Compute at each layer and pass to the next\n",
    "        y_hat = input\n",
    "        # Ensure the input has 2 dimensions to simplify adding the bias column.\n",
    "        if y_hat.ndim == 1:\n",
    "            y_hat = y_hat[np.newaxis, :]\n",
    "        for n in range(self.n_layers):\n",
    "            # Add a column with weights of 1 to be multiplied with the\n",
    "            # bias terms (the final row in the weights matrix)\n",
    "            ones_col = np.ones((y_hat.shape[0], 1), dtype=y_hat.dtype)\n",
    "            y_hat = np.concatenate((y_hat, ones_col), axis=1)\n",
    "            # input array * weights matrix (matrix multiplication)\n",
    "            y_hat = y_hat @ self.params[n+1]\n",
    "            outputs[n+1] = {'input': y_hat}\n",
    "            if self.activation == 'sigmoid':\n",
    "                y_hat = sigmoid(y_hat)\n",
    "            else:\n",
    "                y_hat = relu(y_hat)\n",
    "            outputs[n+1]['activation'] = y_hat\n",
    "        # Finish with softmax if using cross-entropy loss.\n",
    "        if self.loss_fn[0] == 'c':\n",
    "            y_hat = softmax(x_input=y_hat, axis=1)\n",
    "        # Softmax outputs don't need to be stored because I \"skip over\"\n",
    "        # the softmax derivative by combining it with the derivative\n",
    "        # for cross-entropy loss.\n",
    "        # Store output and activation values\n",
    "        self.outputs = outputs\n",
    "        # Return final predictions\n",
    "        return y_hat\n",
    "        \n",
    "    \n",
    "    # Aliases for forward()\n",
    "    __call__ = forward  # Enables calling the object like a function\n",
    "    predict = forward\n",
    "\n",
    "\n",
    "    # Backpropagation algorithm for computing gradients\n",
    "    def backward(self, x_input: np.ndarray, y_target: np.ndarray):\n",
    "        '''Compute a backward pass through the network, storing the partial\n",
    "        derivatives of the parameters with respect to the loss.\n",
    "\n",
    "        Updates the nn.grad property, which holds a list of gradients by layer\n",
    "        in the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_input: np.ndarray\n",
    "            Input to the network, either a single sample or a batch of samples\n",
    "            with shape (m_samples, n_features).\n",
    "        y_target: np.ndarray\n",
    "            The targets to predict. If this is a regression task, the array\n",
    "            will likely be of shape (m_samples,). If it is a classification\n",
    "            task, the target array will be a one-hot coded array of\n",
    "            of shape (m_samples, n_classes), with a `1` in each row for the\n",
    "            correct class.\n",
    "        '''\n",
    "        # Validate inputs\n",
    "        if self.lambda_ < 0:\n",
    "            raise ValueError(\"The regularizing term lambda_ cannot be negative. Set to a float value >= 0.0 with nn.lambda_ = val\")\n",
    "        \n",
    "        if self.alpha < 0:\n",
    "            raise ValueError(\"The learning rate, alpha, cannot be negative. Set to a float value >= 0.0 with nn.alpha = val\")\n",
    "        \n",
    "        num_samples = x_input.shape[0] if x_input.ndim > 1 else 1\n",
    "\n",
    "        # Set the regularization scalar. This is the derivative of L2\n",
    "        # regularization when added to the loss function.\n",
    "        # L2 regularization = (lambda / (2 * num_samples)) * sum(weights ** 2)\n",
    "        # The derivative = (lambda / num_samples) * weights\n",
    "        # The update rule for weights is:\n",
    "        # weights_new = weights - alpha * (gradient + regularizing_derivative)\n",
    "        # weights_new = weights - (alpha * regularizing_derivative) - (alpha * gradient)\n",
    "        # Factor out \"weights\" from the first part:\n",
    "        # weights_new = weights(1 - alpha * (lambda/num_samples))\n",
    "        self.reg_scalar = 1 - (self.alpha * self.lambda_ / num_samples)\n",
    "\n",
    "        # Add regularizing factor to the loss\n",
    "        squared_weights = 0\n",
    "        for n, layer in self.params.items():\n",
    "            squared_weights += np.sum(layer ** 2)\n",
    "        loss = (self.lambda_ * squared_weights) / (2 * num_samples)\n",
    "\n",
    "        y_hat = self.forward(x_input)\n",
    "        \n",
    "        # Loss function: cross_entropy or mean_squared_error\n",
    "        if self.loss_fn[0] == 'c':\n",
    "            loss += cross_entropy_loss(y_hat, y_target)\n",
    "        else:\n",
    "            # mean-squared error\n",
    "            loss += mse_loss(y_hat, y_target)\n",
    "        # Store the loss value\n",
    "        self.loss = loss\n",
    "        \n",
    "        if self.activation == 'sigmoid':\n",
    "            df_activation_fn = df_sigmoid\n",
    "        else:\n",
    "            df_activation_fn = df_relu\n",
    "\n",
    "        # Derivative of cost with respect to activations.\n",
    "        # Calculate the error. For both mean-squared and cross-entropy\n",
    "        # loss, this is the derivative of the loss with respect to the\n",
    "        # final layer's output (after activations, before softmax)\n",
    "        error = (y_hat - y_target) / num_samples\n",
    "\n",
    "        # Store derivatives of activations\n",
    "        df_outputs = {}\n",
    "\n",
    "        layers_reverse_order = list(range(1, self.n_layers + 1))[::-1]\n",
    "        for n in layers_reverse_order:\n",
    "            # Derivative of activations with respect to inputs.\n",
    "            # To get the derivative of the loss with respect to the\n",
    "            # input of the final layer before activations, multiply\n",
    "            # the error with the derivative of the activation function.\n",
    "            # Note: if using a bias array separate from the weight matrix,\n",
    "            # this is the derivative for the bias in the final layer.\n",
    "            if n == max(layers_reverse_order):\n",
    "                # For the final layer, use the derivative of the cost\n",
    "                # with respect to activations (i.e., the error)\n",
    "                df_outputs[n] = {'activation': (\n",
    "                    error \n",
    "                    * df_activation_fn(self.outputs[n]['input'])\n",
    "                )}\n",
    "            else:\n",
    "                # For subsequent layers, use the derivative of the inputs\n",
    "                # with respect to the prior layer activations.\n",
    "                # df_outputs[n]['activation'] = (\n",
    "                #     df_outputs[n]['input'] \n",
    "                #     * df_activation_fn(self.outputs[n]['input'])\n",
    "                # )\n",
    "\n",
    "                df_outputs[n]['activation'] = (\n",
    "                    df_outputs[n]['input'] \n",
    "                    * np.concatenate(\n",
    "                    (df_activation_fn(self.outputs[n]['input']),\n",
    "                     np.ones(shape=(self.outputs[n]['input'].shape[0], 1))\n",
    "                    ), axis=1)\n",
    "                )\n",
    "\n",
    "            # Derivative of inputs with respect to weights.\n",
    "            # To get the derivative of the loss with respect to the weights\n",
    "            # for the final layer, multiply the derivative above with\n",
    "            # the activations from the previous layer.\n",
    "            # NOTE: the gradient ACCUMULATES (+=).\n",
    "            # self.grad[n] += (\n",
    "            #     df_outputs[n]['activation']\n",
    "            #     * self.outputs[n - 1]['activation']\n",
    "            # )\n",
    "\n",
    "            self.grad[n] += (\n",
    "                df_outputs[n]['activation'].T\n",
    "                @ np.concatenate(\n",
    "                    (self.outputs[n - 1]['activation'],\n",
    "                    # Add a column of 1s for the bias term\n",
    "                    np.ones(shape=(self.outputs[n-1]['activation'].shape[0], 1))\n",
    "                    ),\n",
    "                    axis=1)\n",
    "                ).T\n",
    "\n",
    "\n",
    "            # Derivative of inputs with respect to prior layer activations.\n",
    "            # To get the derivative of the loss with respect to the prior\n",
    "            # layer's outputs (activations), multiply the {derivative of\n",
    "            # the activations with respect to the inputs} by the {weights}.\n",
    "            # df_outputs[n-1] = {'input': (\n",
    "            #     df_outputs[n]['activation']\n",
    "            #     * self.params[n]\n",
    "            # )}\n",
    "\n",
    "            df_outputs[n-1] = {'input': (\n",
    "                df_outputs[n]['activation']\n",
    "                @ self.params[n].T\n",
    "            )}\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        '''Reset all gradients to 0.'''\n",
    "        for layer in self.grad:\n",
    "            layer.fill(0)\n",
    "        return None\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        '''Update the weights, taking a single step in the direction of the\n",
    "        negative gradient, scaled by the learning_rate.\n",
    "\n",
    "        Note: call nn.zero_grad() to reset the gradients after taking a step,\n",
    "        otherwise gradients are accumulated with each backward pass. \n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The step uses nn.lambda_, the weighting factor for the\n",
    "        regularizing term in the loss calculation. If set to 0.0, no\n",
    "        regularization will be used. The larger this value, the more weights\n",
    "        are penalized and the smaller the weights will be after training\n",
    "        through gradient descent. The default for lambda_ is 0.0, which means\n",
    "        that no regularization will be applied.\n",
    "        '''\n",
    "        \n",
    "        for n in range(1, self.n_layers + 1):\n",
    "            self.params[n] = (\n",
    "                # Reduce the weights according to the regularizing setting\n",
    "                (self.params[n] * self.reg_scalar)\n",
    "                # Take a scaled step in the direction of the negative gradient\n",
    "                - self.alpha * self.grad[n]\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos\n",
    "This section demonstrates how to use the network for classification and regression tasks. See the [Word embeddings](#word-embeddings) section of this notebook for a more sophisticated use of this network trained on a classification task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn(\n",
    "    input_dim = 10,\n",
    "    n_layers = 4,\n",
    "    neurons_per_layer = [3, 4, 6, 2],\n",
    "    bias = True,\n",
    "    activation = 'relu',\n",
    "    lambda_ = 0.0,\n",
    "    loss_fn = 'cross_entropy',\n",
    "    alpha = 0.001,\n",
    "    dtype = np.float32,\n",
    "    seed = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: (11, 3)\n",
      "Parameters: (4, 4)\n",
      "Parameters: (5, 6)\n",
      "Parameters: (7, 2)\n",
      "Total parameters: 93\n",
      "Total count: 93\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for n in range(model.n_layers):\n",
    "    print(f\"Parameters: {model.params[n+1].shape}\")\n",
    "    m, n = model.params[n+1].shape\n",
    "    count += (m * n)\n",
    "print(f\"Total parameters: {model.n_params:,.0f}\")\n",
    "print(f\"Total count: {count:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "[[0.5        0.5       ]\n",
      " [0.75544368 0.24455632]\n",
      " [0.95906044 0.04093956]]\n",
      "Targets:\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Loss:\n",
      "11.609628918786017\n"
     ]
    }
   ],
   "source": [
    "x_input = np.arange(30).reshape((3, 10))\n",
    "y_target = np.array([\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0]\n",
    "])\n",
    "y_hat = model.predict(x_input)\n",
    "print(f\"Predictions:\\n{y_hat}\")\n",
    "print(f\"Targets:\\n{y_target}\")\n",
    "loss = cross_entropy_loss(y_hat, y_target)\n",
    "print(f\"Loss:\\n{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,6) (5,7) (5,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ryan\\OneDrive\\Code\\ml-projects\\neural_network_from_scratch.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mbackward(x_input, y_target)\n",
      "\u001b[1;32mc:\\Users\\Ryan\\OneDrive\\Code\\ml-projects\\neural_network_from_scratch.ipynb Cell 13\u001b[0m in \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=384'>385</a>\u001b[0m     df_outputs[n][\u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=385'>386</a>\u001b[0m         df_outputs[n][\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m] \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=386'>387</a>\u001b[0m         \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mconcatenate(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=389'>390</a>\u001b[0m         ), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=390'>391</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=392'>393</a>\u001b[0m \u001b[39m# Derivative of inputs with respect to weights.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=393'>394</a>\u001b[0m \u001b[39m# To get the derivative of the loss with respect to the weights\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=394'>395</a>\u001b[0m \u001b[39m# for the final layer, multiply the derivative above with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=399'>400</a>\u001b[0m \u001b[39m#     * self.outputs[n - 1]['activation']\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=400'>401</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=402'>403</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad[n] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=403'>404</a>\u001b[0m     df_outputs[n][\u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mT\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=404'>405</a>\u001b[0m     \u001b[39m@\u001b[39m np\u001b[39m.\u001b[39mconcatenate(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=405'>406</a>\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs[n \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=406'>407</a>\u001b[0m         \u001b[39m# Add a column of 1s for the bias term\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=407'>408</a>\u001b[0m         np\u001b[39m.\u001b[39mones(shape\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs[n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=408'>409</a>\u001b[0m         ),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=409'>410</a>\u001b[0m         axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=410'>411</a>\u001b[0m     )\u001b[39m.\u001b[39mT\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=413'>414</a>\u001b[0m \u001b[39m# Derivative of inputs with respect to prior layer activations.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=414'>415</a>\u001b[0m \u001b[39m# To get the derivative of the loss with respect to the prior\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=415'>416</a>\u001b[0m \u001b[39m# layer's outputs (activations), multiply the {derivative of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=419'>420</a>\u001b[0m \u001b[39m#     * self.params[n]\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=420'>421</a>\u001b[0m \u001b[39m# )}\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=422'>423</a>\u001b[0m df_outputs[n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m: (\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=423'>424</a>\u001b[0m     df_outputs[n][\u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=424'>425</a>\u001b[0m     \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[n]\u001b[39m.\u001b[39mT\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ryan/OneDrive/Code/ml-projects/neural_network_from_scratch.ipynb#X15sZmlsZQ%3D%3D?line=425'>426</a>\u001b[0m )}\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,6) (5,7) (5,6) "
     ]
    }
   ],
   "source": [
    "model.backward(x_input, y_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for next steps\n",
    "Perform an embedding of products for market basket analysis. For example, see this [Kaggle dataset](https://www.kaggle.com/datasets/heeraldedhia/groceries-dataset?select=Groceries_dataset.csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Older version of the network, with weights and biases stored separately. Backpropagation not yet implemented for this version of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn():\n",
    "    '''Create a fully connected neural network with a configurable number of layers and neurons.'''\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim: int,\n",
    "            n_layers: int = 2, \n",
    "            neurons_per_layer: Union[int, list[int]] = [3, 1], \n",
    "            bias: Union[bool, list[bool]] = True,\n",
    "            lambda_: float = 0.0,\n",
    "            loss_fn: str = 'cross_entropy',\n",
    "            seed: Union[int, None] = None):\n",
    "        '''Instantiate a neural network as a model object that performs\n",
    "        computations on input data and returns the outputs\n",
    "        of those computations. Parameters (weights and biases) are\n",
    "        intitialized to random values with mean 0 and standard deviation of 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            The dimension of the inputs to the model (i.e, the number\n",
    "            of features per input sample).\n",
    "        n_layers: int, default = 2\n",
    "            The number of layers in the network including all hidden layers\n",
    "            and the output layer.\n",
    "        neurons_per_layer: {int, list[int]}, default = [3, 1]\n",
    "            The number of neurons in each layer of the network. If an `int`,\n",
    "            then all layers will have the same number of neurons. If a\n",
    "            `list`, then its `len()` must equal `n_layers`, where each\n",
    "            element in the list holds the number of neurons in the \n",
    "            corresponding layer of the network.\n",
    "        bias: {bool, list[bool]}, default = True\n",
    "            Whether to include a bias term for the neurons in the network.\n",
    "            If `True`, all neurons in the network will have a bias term.\n",
    "            If a list of `bool` values is provided, each layer of the network\n",
    "            will (or won't) have a bias term for its neurons based on the\n",
    "            corresponding element in the list. If a list, the number of\n",
    "            elements must equal `n_layers`.\n",
    "        lambda_: float in the range [0.0, inf)\n",
    "            The weighting factor for the regularizing\n",
    "            term in the loss calculation. If set to 0.0,\n",
    "            no regularization will be used. The larger\n",
    "            this value, the more weights are penalized\n",
    "            and the smaller the weights will be after\n",
    "            training through gradient descent.\n",
    "        loss_fn: {'cross_entropy', 'ce', 'mean_squared_error', 'mse'}\n",
    "            The loss function used for the network. Valid values are\n",
    "            'cross_entropy' or 'mean_squared_error' or an abbreviation.\n",
    "            Use 'mean_squared_error' for regression tasks and set the\n",
    "            output layer to a single neuron.\n",
    "            Use 'cross_entropy' for classification tasks and set\n",
    "            the number of output neurons to the number of possible classes.\n",
    "            Softmax is automatically applied after the output layer when\n",
    "            using 'cross_entropy' loss.\n",
    "        seed: int\n",
    "            For reproducibility, set a seed that will be used\n",
    "            when initializing parameter values. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: an instance of `nn` with randomly-initialized weights.\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        forward(input): compute a forward pass through the network,\n",
    "            returning the output from the final layer.\n",
    "        backward(): compute a backward pass through the network,\n",
    "            storing the partial derivatives of the parameters\n",
    "            with respect to the loss.\n",
    "        zero_grad(): clears (zeros-out) the stored gradients.\n",
    "        step(learning_rate): update the parameters by taking\n",
    "            a step (scaled by learning_rate) in the negative\n",
    "            direction of the gradient.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        w: a list of the network's weights by layer\n",
    "        b: a list of the network's bias values by layer\n",
    "        n_params: the total number of parameters in the network,\n",
    "            including weights and biases.\n",
    "        w_grad: a list of the gradients for each of the weights\n",
    "            in the network, computed after calling backward().\n",
    "        b_grad: a list of the gradients for each of the bias values\n",
    "            in the network, computed after calling backward().\n",
    "        activations: a list of the intermediate outputs after each layer,\n",
    "            including the inputs. Used for backpropagation.\n",
    "        input_dim: the size of the input that will be passed into the\n",
    "            network. The network will expect all inputs to have this\n",
    "            same dimension. Since each input sample is a vector, this\n",
    "            is the number of features in an input row.\n",
    "        n_layers: The number of layers in the network, including hidden\n",
    "            layers and the output layer (excluding the input).\n",
    "        neurons_per_layer: An integer or a list of integers representing\n",
    "            the number of neurons in each layer.\n",
    "        bias: A boolean value or a list of boolean values representing\n",
    "            whether each layer has a bias term added to it.\n",
    "        '''\n",
    "        \n",
    "        # Validate arguments\n",
    "        if type(input_dim) != int:\n",
    "            raise TypeError(f\"input_dim must be an integer, but the provided value was: {type(input_dim)}\")\n",
    "\n",
    "        if type(n_layers) != int:\n",
    "            raise TypeError(f\"n_layers must be an integer, but the provided value was: {type(n_layers)}\")\n",
    "        \n",
    "        if type(neurons_per_layer) not in (int, list):\n",
    "            raise TypeError(f\"neurons_per_layer must be an int or a list of int, not: {type(neurons_per_layer)}\")\n",
    "        elif type(neurons_per_layer) == list:\n",
    "            if len(neurons_per_layer) != n_layers:\n",
    "                raise ValueError(f\"If neurons_per_layer is a list, it must have the same number of elements as n_layers ({n_layers}).\")\n",
    "            if any([type(i) != int for i in neurons_per_layer]):\n",
    "                raise TypeError(\"neurons_per_layer must be an int or a list of int. Not all elements provided in neurons_per_layer were of the int type.\")\n",
    "        \n",
    "        if type(bias) not in (bool, list):\n",
    "            raise TypeError(f\"bias must be a bool or a list of bool, not {type(bias)}\")\n",
    "        elif type(bias) == list:\n",
    "            if len(bias) != n_layers:\n",
    "                raise ValueError(f\"If bias is a list, it must have the same number of elements as n_layers ({n_layers}).\")\n",
    "            if any([type(i) != bool for i in bias]):\n",
    "                raise TypeError(\"bias must be a bool or a list of bool. Not all elements provided in bias were of the bool type.\")\n",
    "        \n",
    "        if type(loss_fn) != str:\n",
    "            raise TypeError(f\"loss_fn must be a str value, not {type(loss_fn)}\")\n",
    "        elif loss_fn[0] not in ['c', 'm']:\n",
    "            raise ValueError(f\"loss_fn must be 'cross_entropy', 'mean_squared_error', or an abbreviation, not {loss_fn}\")\n",
    "\n",
    "        if type(seed) not in [int, None]:\n",
    "            raise TypeError(f\"Seed must be an int value, not {type(seed)}\")\n",
    "\n",
    "        # Update object's parameters (equivalent to self.n_layers = n_layers; self.bias = bias; ...)\n",
    "        self.__dict__.update(locals())\n",
    "\n",
    "        # If neurons_per_layer and bias are individual values, convert them \n",
    "        # to lists to use when creating layers\n",
    "        if type(neurons_per_layer) == int:\n",
    "            neurons_per_layer = [neurons_per_layer for _ in range(n_layers)]\n",
    "        if type(bias) == bool:\n",
    "            bias = [bias for _ in range(n_layers)]\n",
    "    \n",
    "        # Create layers (parameters)\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        input_sizes = [input_dim]\n",
    "        input_sizes.extend(neurons_per_layer[:-1])\n",
    "        for n in range(n_layers):\n",
    "            w, b = self._linear_layer(\n",
    "                input_sizes[n], neurons_per_layer[n], bias[n], seed=seed)\n",
    "            self.w.append(w)\n",
    "            self.b.append(b)\n",
    "        \n",
    "        # The second expression counts bias terms only for layers with a bias\n",
    "        self.n_params = (np.dot(input_sizes, neurons_per_layer) \n",
    "                         + np.dot(bias, neurons_per_layer))\n",
    "\n",
    "\n",
    "    def _linear_layer(self, input_size: int, n_neurons: int, bias: bool, seed: Union[int, None]):\n",
    "        '''Create a linear layer. Used when constructing the network\n",
    "        at time of instantiation.\n",
    "        \n",
    "        The input size determines the number of weights (rows) and the\n",
    "        number of neurons determines the number of columns. The bias\n",
    "        will be a 1D NumPy array with len() equal to the n_neurons.\n",
    "        '''\n",
    "        # set up random number generator\n",
    "        rng = np.random.default_rng(seed)\n",
    "        # Generate the weight and bias arrays\n",
    "        w = rng.normal(size=(input_size, n_neurons))\n",
    "        if bias:\n",
    "            b = rng.normal(size=(n_neurons))\n",
    "        else:\n",
    "            b = np.zeros(shape=(n_neurons))\n",
    "        return w, b\n",
    "\n",
    "\n",
    "    def forward(self, input: np.ndarray, activation: str='relu'):\n",
    "        '''Pass inputs through the network and return the outputs from\n",
    "        the final network layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input: np.ndarray\n",
    "            Input to the network, either a single sample or a batch of samples\n",
    "            with shape: m_samples, n_features_per_sample.\n",
    "        activation: {'relu', 'sigmoid'}, default='relu'\n",
    "            The activation function to use after passing input through each\n",
    "            linear layer.\n",
    "        '''\n",
    "        # Validate input\n",
    "        input_size = input.shape[1] if input.ndim > 1 else input.shape[0]\n",
    "        if input_size != self.input_dim:\n",
    "            raise ValueError(f\"Passed input of size {input_size} does not match\"\n",
    "                             f\" expected size of input_dim: {self.input_dim}.\")\n",
    "        \n",
    "        # Compute at each layer and pass to the next\n",
    "        y_hat = input\n",
    "        for n in range(self.n_layers):\n",
    "            y_hat = linear(x_input=y_hat, weights=self.w[n], bias=self.b[n])\n",
    "            if activation == 'sigmoid':\n",
    "                y_hat = sigmoid(y_hat)\n",
    "            else:\n",
    "                y_hat = relu(y_hat)        \n",
    "        return y_hat\n",
    "        \n",
    "    \n",
    "    # Aliases for forward()\n",
    "    __call__ = forward  # Enables calling the object like a function\n",
    "    predict = forward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # weights: list of weight layers, arrays of shape (x_inputs, n_neurons)\n",
    "    #     The network's weight layers, used for computing\n",
    "    #     the regularized loss, which penalizes large weights.\n",
    "    # lambda_: float in the range [0.0, inf)\n",
    "    #     The weighting factor for the regularizing\n",
    "    #     term in the loss calculation. If set to 0.0,\n",
    "    #     no regularization will be used. The larger\n",
    "    #     this value, the more weights are penalized\n",
    "    #     and the smaller the weights will be after\n",
    "    #     training through gradient descent.\n",
    "\n",
    "    # # Add regularization\n",
    "    # squared_weights = 0\n",
    "    # for layer in weights:\n",
    "    #     squared_weights += np.sum(layer ** 2)\n",
    "    # loss += (lambda_ * squared_weights) / (2 * num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn(input_dim=10, n_layers=4, neurons_per_layer=[3, 4, 6, 2], bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(model.n_layers):\n",
    "    print(f\"Weights: {model.w[n].shape}, bias: {model.b[n].shape}\")\n",
    "print(f\"Total parameters: {model.n_params:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = np.arange(30).reshape(3, 10)\n",
    "model.forward(x_input, activation='sigmoid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a1d72d1d2bd69df5fbb242d8d66be7c2c6ff0c2599bcae97545e6959cba1f2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
